1、如何实现长期依赖？

在一个训练好的网络中，当输入序列没有重要信息时，LSTM遗忘门的值接近为1，输入门接近0，此时过去的记忆会被保存，从而实现了长期记忆；当输入的序列中出现了重要信息时，LSTM会将其存入记忆中，此时输入门的值会接近于1；当输入序列出现重要信息，且该信息意味着之前的记忆不再重要的时候，输入门接近1，遗忘门接近0，这样旧的记忆被遗忘，新的重要信息被记忆。经过这样的设计，整个网络更容易学习到序列之间的长期依赖。


2、如何避免梯度消失/爆炸？

在lstm中，状态c是通过累加的方式来计算的。不像RNN中的累乘的形式，这样的话，它的的导数也不是乘积的形式，这样就不会发生梯度消失的情况了。